{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np, tensorflow as tf\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport pandas as pd, os, cv2\nimport tensorflow as tf, pydicom as dicom\nfrom tensorflow.keras import layers, models\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport sklearn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-30T15:51:15.929368Z","iopub.execute_input":"2023-09-30T15:51:15.929697Z","iopub.status.idle":"2023-09-30T15:51:15.935530Z","shell.execute_reply.started":"2023-09-30T15:51:15.929674Z","shell.execute_reply":"2023-09-30T15:51:15.934361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:16.036742Z","iopub.execute_input":"2023-09-30T15:51:16.037520Z","iopub.status.idle":"2023-09-30T15:51:16.044970Z","shell.execute_reply.started":"2023-09-30T15:51:16.037489Z","shell.execute_reply":"2023-09-30T15:51:16.044045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBIG_TRAIN = '/kaggle/input/rsna-make-tfrec/train_images/train_images.npz'\nBIG_VALID = '/kaggle/input/rsna-make-tfrec/valid_images/valid_images.npz'\nMEDIUM_TRAIN = '/kaggle/input/rsna-make-tfrec/mediumtrain_images/mediumtrain_images.npz'\nMEDIUM_VALID = '/kaggle/input/rsna-make-tfrec/mediumvalid_images/mediumvalid_images.npz'\nSMALL_TRAIN = '/kaggle/input/rsna-make-tfrec/smalltrain_images/smalltrain_images.npz'\nSMALL_VALID = '/kaggle/input/rsna-make-tfrec/smallvalid_images/smallvalid_images.npz'\nBATCH_SIZE = 32 #* strategy.num_replicas_in_sync\nIMG_SIZE = [224, 224]\nSHUFFLE_BUFFER_SIZE = 60\nEPOCHS = 15\nTARGET_COLS  = [\n        \"bowel_injury\", \"extravasation_injury\",\n        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n        \"liver_healthy\", \"liver_low\", \"liver_high\",\n        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n    ]\nORIGINAL_COLS = [\n        \"bowel_healthy\", \"bowel_injury\", \"extravasation_healthy\", \"extravasation_injury\",\n        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n        \"liver_healthy\", \"liver_low\", \"liver_high\",\n        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n    ]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:16.046897Z","iopub.execute_input":"2023-09-30T15:51:16.047489Z","iopub.status.idle":"2023-09-30T15:51:16.060903Z","shell.execute_reply.started":"2023-09-30T15:51:16.047459Z","shell.execute_reply":"2023-09-30T15:51:16.059953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = MEDIUM_TRAIN\nwith np.load(train_path, allow_pickle = True) as data:\n    train_images = data['images']\n    train_labels = data['labels']\nDATA_SIZE = np.shape(train_images)[0]\nprint(DATA_SIZE)\n\nvalid_path = MEDIUM_VALID\nwith np.load(valid_path, allow_pickle = True) as data:\n    valid_images = data['images']\n    valid_labels = data['labels']\nVALID_DATA_SIZE = np.shape(valid_images)[0]\n\ndef decode_label(image, label):\n    label = tf.cast(label, tf.float32)\n    #         bowel       fluid       kidney      liver       spleen\n    labels = (label[0:1], label[1:2], label[2:5], label[5:8], label[8:11])\n    \n    return (image, labels)\n\ndef apply_augmentation(images, labels):\n    augmenter = keras_cv.layers.Augmenter(\n        [\n            #keras_cv.layers.RandomFlip(mode=\"horizontal\"),\n            tf.keras.layers.RandomBrightness(factor = 0.2),#, value_range=(0, 1)),\n            tf.keras.layers.RandomContrast(factor=0.2),\n            #tf.keras.layers.RandomZoom(height_factor=0.1, width_factor=0.1),\n            tf.keras.layers.LayerNormalization(),\n            #keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n        ]\n    )\n    return (augmenter(images), labels)\n\ntrain_dataset = (tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n                 .map(decode_label, num_parallel_calls=AUTOTUNE)\n                 .shuffle(SHUFFLE_BUFFER_SIZE)\n                 .batch(BATCH_SIZE))\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((valid_images, valid_labels))\n                 .map(decode_label, num_parallel_calls=AUTOTUNE)\n                 .batch(BATCH_SIZE))","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:16.062276Z","iopub.execute_input":"2023-09-30T15:51:16.062770Z","iopub.status.idle":"2023-09-30T15:51:26.945530Z","shell.execute_reply.started":"2023-09-30T15:51:16.062742Z","shell.execute_reply":"2023-09-30T15:51:26.944484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    # Define Input\n    inputs = keras.Input(shape=(224, 224, 1) )#IMAGE_SIZE + [1,], batch_size=BATCH_SIZE)\n    \n    #x = layers.Conv2D(3, (1, 1), activation='relu')(inputs)\n    \n    def expand_channels(x):\n        x = tf.image.grayscale_to_rgb(x)\n        return x\n\n    # Apply the function to the input\n    rgb_image = keras.layers.Lambda(expand_channels)(inputs)\n     \n    # Define Backbone\n    #hub.KerasLayer(\"https://www.kaggle.com/models/tensorflow/resnet-50/frameworks/TensorFlow2/variations/classification/versions/1\")\n    backbone = tf.keras.applications.Xception(weights='imagenet', input_shape=(224,224, 3), include_top=False)\n    x = backbone(rgb_image)#(x)\n    \n    # GAP to get the activation maps\n    gap = keras.layers.GlobalAveragePooling2D()\n    x = gap(x)\n\n    # Define 'necks' for each head\n    x_bowel = keras.layers.Dense(32, activation='relu', name = 'bowel_neck')(x)\n    x_extra = keras.layers.Dense(32, activation='relu', name = 'extra_neck')(x)\n    x_liver = keras.layers.Dense(32, activation='relu', name = 'liver_neck')(x)\n    x_kidney = keras.layers.Dense(32, activation='relu', name = 'kidney_neck')(x)\n    x_spleen = keras.layers.Dense(32, activation='relu', name = 'spleen_neck')(x)\n\n    # Define heads\n    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n    \n    # Concatenate the outputs\n    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n\n    # Create model\n    print(\"[INFO] Building the model...\")\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n        # Compile the model\n    optimizer = keras.optimizers.Adam()\n    loss = {\n        \"bowel\": keras.losses.BinaryCrossentropy(),\n        \"extra\": keras.losses.BinaryCrossentropy(),\n        \"liver\": keras.losses.CategoricalCrossentropy(),\n        \"kidney\": keras.losses.CategoricalCrossentropy(),\n        \"spleen\": keras.losses.CategoricalCrossentropy(),\n    }\n    metrics = {\n        \"bowel\": 'accuracy',  # Change to accuracy for binary classification\n        \"extra\": 'accuracy',  # Change to accuracy for binary classification\n        \"liver\": 'accuracy',  # Change to accuracy for multi-class classification\n        \"kidney\": 'accuracy',  # Change to accuracy for multi-class classification\n        \"spleen\": 'accuracy',  # Change to accuracy for multi-class classification\n    }\n\n    print(\"[INFO] Compiling the model...\")\n    model.compile(\n        optimizer=optimizer,\n        loss=loss,\n        metrics=metrics,) \n        #loss_weights={'bowel': 2.0, 'extra': 6.0, 'liver': 2.0, 'kidney': 2.0, 'spleen': 2.0})\n    \n    return model\n\n# build the model\nprint(\"[INFO] Building the model...\")\nmodel = build_model()\n\nprint(model.summary())\n\n# Create early stopping callback\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.1, patience=10)\nnum_steps = (DATA_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n\n# train\nprint(\"[INFO] Training...\")\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=num_steps,\n    epochs=EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=[early_stop],\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:52:25.721485Z","iopub.execute_input":"2023-09-30T15:52:25.721805Z","iopub.status.idle":"2023-09-30T15:54:39.652502Z","shell.execute_reply.started":"2023-09-30T15:52:25.721781Z","shell.execute_reply":"2023-09-30T15:54:39.651012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 3x2 grid for the subplots\nfig, axes = plt.subplots(5, 1, figsize=(5, 15))\n\n# Flatten axes to iterate through them\naxes = axes.flatten()\n\n# Iterate through the metrics and plot them\nfor i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n    # Plot training accuracy\n    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n    # Plot validation accuracy\n    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n    axes[i].set_title(name)\n    axes[i].set_xlabel('Epoch')\n    axes[i].set_ylabel('Accuracy')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\nplt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:28.836872Z","iopub.status.idle":"2023-09-30T15:51:28.837797Z","shell.execute_reply.started":"2023-09-30T15:51:28.837559Z","shell.execute_reply":"2023-09-30T15:51:28.837583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store best results\nbest_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nbest_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\nbest_acc_extra = history.history['val_extra_accuracy'][best_epoch]\nbest_acc_liver = history.history['val_liver_accuracy'][best_epoch]\nbest_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\nbest_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n\n# Find mean accuracy\nbest_acc = np.mean(\n    [best_acc_bowel,\n     best_acc_extra,\n     best_acc_liver,\n     best_acc_kidney,\n     best_acc_spleen\n])\n\n\nprint(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\nprint('ORGAN Acc:')\nprint(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\nprint(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\nprint(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\nprint(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\nprint(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:28.838996Z","iopub.status.idle":"2023-09-30T15:51:28.839880Z","shell.execute_reply.started":"2023-09-30T15:51:28.839650Z","shell.execute_reply":"2023-09-30T15:51:28.839672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save(\"rsna-atd.keras\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:28.841198Z","iopub.status.idle":"2023-09-30T15:51:28.841607Z","shell.execute_reply.started":"2023-09-30T15:51:28.841400Z","shell.execute_reply":"2023-09-30T15:51:28.841421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_proc(pred):\n    proc_pred = np.empty((len(pred[0]), 2*2 + 3*3), dtype=\"float32\")\n\n    # bowel, extravasation\n    proc_pred[:,0] = np.transpose(np.array(pred[:][0]))\n    proc_pred[:, 1] = 1 - proc_pred[:, 0]\n    proc_pred[:, 2] = np.transpose(np.array(pred[:][1]))\n    proc_pred[:, 3] = 1 - proc_pred[:, 2]\n    \n    # liver, kidney, sneel\n    proc_pred[:, 4:7] = pred[:][2]\n    proc_pred[:, 7:10] = pred[:][3]\n    proc_pred[:, 10:13] = pred[:][4]\n\n    return proc_pred\n\npredictions = pd.DataFrame(post_proc(model.predict(valid_dataset)))\npredictions.columns = ORIGINAL_COLS\ndisplay(predictions)\nvalidation = pd.DataFrame(valid_labels)\nvalidation.columns = TARGET_COLS\nvalidation['bowel_healthy'] = 1 - validation['bowel_injury']\nvalidation['any_injury'] = 1\nvalidation['extravasation_healthy'] = 1 - validation['extravasation_injury']\ndisplay(validation)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:28.842772Z","iopub.status.idle":"2023-09-30T15:51:28.843674Z","shell.execute_reply.started":"2023-09-30T15:51:28.843447Z","shell.execute_reply":"2023-09-30T15:51:28.843480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Score](https://www.kaggle.com/code/jakebrusca/rsna23-weighted-mean-baseline)","metadata":{}},{"cell_type":"code","source":"# List of Targets\nInjuries = ['bowel_healthy', 'bowel_injury', \n            'extravasation_healthy', 'extravasation_injury', \n            'kidney_healthy', 'kidney_low', 'kidney_high', \n            'liver_healthy', 'liver_low', 'liver_high', \n            'spleen_healthy', 'spleen_low', 'spleen_high', \n            'any_injury']\n\n# Group by different sample weights\nscale_by_2 = ['bowel_injury','kidney_low','liver_low','spleen_low']\nscale_by_4 = ['kidney_high','liver_high','spleen_high']\nscale_by_6 = ['extravasation_injury','any_injury']\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef normalize_probabilities_to_one(df: pd.DataFrame, group_columns: list) -> pd.DataFrame:\n    # Normalize the sum of each row's probabilities to 100%.\n    # 0.75, 0.75 => 0.5, 0.5\n    # 0.1, 0.1 => 0.5, 0.5\n    row_totals = df[group_columns].sum(axis=1)\n    if row_totals.min() == 0:\n        raise ParticipantVisibleError('All rows must contain at least one non-zero prediction')\n    for col in group_columns:\n        df[col] /= row_totals\n    return df\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    '''\n    Pseudocode:\n    1. For every label group (liver, bowel, etc):\n        - Normalize the sum of each row's probabilities to 100%.\n        - Calculate the sample weighted log loss.\n    2. Derive a new any_injury label by taking the max of 1 - p(healthy) for each label group\n    3. Calculate the sample weighted log loss for the new label group\n    4. Return the average of all of the label group log losses as the final score.\n    '''\n    print(type(solution))\n    #del solution[row_id_column_name]\n    #del submission[row_id_column_name]\n\n    # Run basic QC checks on the inputs\n    if not pd.api.types.is_numeric_dtype(submission.values):\n        raise ParticipantVisibleError('All submission values must be numeric')\n\n    if not np.isfinite(submission.values).all():\n        raise ParticipantVisibleError('All submission values must be finite')\n\n    if solution.min().min() < 0:\n        raise ParticipantVisibleError('All labels must be at least zero')\n    if submission.min().min() < 0:\n        raise ParticipantVisibleError('All predictions must be at least zero')\n\n    # Calculate the label group log losses\n    binary_targets = ['bowel', 'extravasation']\n    triple_level_targets = ['kidney', 'liver', 'spleen']\n    all_target_categories = binary_targets + triple_level_targets\n\n    label_group_losses = []\n    for category in all_target_categories:\n        if category in binary_targets:\n            col_group = [f'{category}_healthy', f'{category}_injury']\n        else:\n            col_group = [f'{category}_healthy', f'{category}_low', f'{category}_high']\n\n        solution = normalize_probabilities_to_one(solution, col_group)\n\n        for col in col_group:\n            if col not in submission.columns:\n                raise ParticipantVisibleError(f'Missing submission column {col}')\n        submission = normalize_probabilities_to_one(submission, col_group)\n        label_group_losses.append(\n            sklearn.metrics.log_loss(\n                y_true=solution[col_group].values,\n                y_pred=submission[col_group].values,\n                sample_weight=solution[f'{category}_weight'].values\n            )\n        )\n\n    # Derive a new any_injury label by taking the max of 1 - p(healthy) for each label group\n    healthy_cols = [x + '_healthy' for x in all_target_categories]\n    any_injury_labels = (1 - solution[healthy_cols]).max(axis=1)\n    any_injury_predictions = (1 - submission[healthy_cols]).max(axis=1)\n    any_injury_loss = sklearn.metrics.log_loss(\n        y_true=any_injury_labels.values,\n        y_pred=any_injury_predictions.values,\n        sample_weight=solution['any_injury_weight'].values\n    )\n\n    label_group_losses.append(any_injury_loss)\n    return np.mean(label_group_losses)\n\n# Assign the appropriate weights to each category\ndef create_training_solution(y_train):\n    sol_train = y_train.copy()\n    \n    # bowel healthy|injury sample weight = 1|2\n    sol_train['bowel_weight'] = np.where(sol_train['bowel_injury'] == 1, 2, 1)\n    \n    # extravasation healthy/injury sample weight = 1|6\n    sol_train['extravasation_weight'] = np.where(sol_train['extravasation_injury'] == 1, 6, 1)\n    \n    # kidney healthy|low|high sample weight = 1|2|4\n    sol_train['kidney_weight'] = np.where(sol_train['kidney_low'] == 1, 2, np.where(sol_train['kidney_high'] == 1, 4, 1))\n    \n    # liver healthy|low|high sample weight = 1|2|4\n    sol_train['liver_weight'] = np.where(sol_train['liver_low'] == 1, 2, np.where(sol_train['liver_high'] == 1, 4, 1))\n    \n    # spleen healthy|low|high sample weight = 1|2|4\n    sol_train['spleen_weight'] = np.where(sol_train['spleen_low'] == 1, 2, np.where(sol_train['spleen_high'] == 1, 4, 1))\n    \n    # any healthy|injury sample weight = 1|6\n    sol_train['any_injury_weight'] = np.where(sol_train['any_injury'] == 1, 6, 1)\n    return sol_train\n\n# Update scale factors to improve score\nsf_2 = 2\nsf_4 = 4\nsf_6 = 14\n\n# The score function deletes the ID column so we remake it\nsolution_train = create_training_solution(validation)\n\n# Reset the prediction, again\npredictions = validation.copy()\npredictions[Injuries] = validation[Injuries].mean().tolist()\n\n# Scale each target \npredictions[scale_by_2] *=sf_2\npredictions[scale_by_4] *=sf_4\npredictions[scale_by_6] *=sf_6\n\nimproved_scale_score = score(solution_train,predictions,'patient_id')\nprint(f'Training score with better scaling: {improved_scale_score}')\n\n#val_score = score(solution = validation, submission = predictions, row_id_column_name = 'patient_id')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T15:51:28.845779Z","iopub.status.idle":"2023-09-30T15:51:28.846172Z","shell.execute_reply.started":"2023-09-30T15:51:28.845972Z","shell.execute_reply":"2023-09-30T15:51:28.845990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Credits \n\nhttps://www.kaggle.com/code/metric/rsna-trauma-metric/notebook\n\nhttps://keras.io/examples/keras_recipes/tfrecord/\n\nhttps://www.kaggle.com/code/amyjang/tensorflow-pneumonia-classification-on-x-rays\n\nhttps://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer\n\nhttps://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}